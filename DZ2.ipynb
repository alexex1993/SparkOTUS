{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7242cac6-59e2-4d1f-b0eb-03dd7409358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63c37f32-eaaa-4f1b-9cf7-41e28182913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/hdfs-cli-example2': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4e66e8b-3e3a-4528-bfec-1eac1c064505",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put bank_transactions_data_2.csv /tmp/hdfs-cli-example2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223a8998-20e6-4e12-8cb6-0c914b9e1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 jupyter supergroup     344980 2024-12-04 12:34 /tmp/hdfs-cli-example2/bank_transactions_data_2.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /tmp/hdfs-cli-example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90c2bf70-cd19-42be-b301-620b445816c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28cf2f65-945f-41bc-8c2f-bdbd997b0855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9922bd40-f287-49a9-9b17-4cddd1cd0145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE transactions (\n",
    "    TransactionID STRING,\n",
    "    AccountID STRING,\n",
    "    TransactionAmount DECIMAL(10,2),\n",
    "    TransactionDate TIMESTAMP,\n",
    "    TransactionType STRING,\n",
    "    Location STRING,\n",
    "    DeviceID STRING,\n",
    "    IP_Address STRING,\n",
    "    MerchantID STRING,\n",
    "    Channel STRING,\n",
    "    CustomerAge INT,\n",
    "    CustomerOccupation STRING,\n",
    "    TransactionDuration INT,\n",
    "    LoginAttempts INT,\n",
    "    AccountBalance DECIMAL(10,2),\n",
    "    PreviousTransactionDate TIMESTAMP\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "    \"separatorChar\" = \",\"\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION \"/tmp/hdfs-cli-example2/\"\n",
    "TBLPROPERTIES (\"skip.header.line.count\" = \"1\");\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "651684d2-61d8-456d-8d90-b049f57c949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /tmp/dz2/transactions_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13d8c215-97e3-471c-8d09-59de72d82b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "CREATE TABLE transactions_partitioned (\n",
    "    TransactionID STRING,\n",
    "    AccountID STRING,\n",
    "    TransactionAmount DECIMAL(10,2),\n",
    "    TransactionDate TIMESTAMP,\n",
    "    TransactionType STRING,\n",
    "    Location STRING,\n",
    "    DeviceID STRING,\n",
    "    IP_Address STRING,\n",
    "    MerchantID STRING,\n",
    "    Channel STRING,\n",
    "    CustomerAge INT,\n",
    "    CustomerOccupation STRING,\n",
    "    TransactionDuration INT,\n",
    "    LoginAttempts INT,\n",
    "    AccountBalance DECIMAL(10,2),\n",
    "    PreviousTransactionDate TIMESTAMP\n",
    ")\n",
    "PARTITIONED BY (year INT)\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/tmp/dz2/transactions_part';\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d7311bc-d833-4bdf-83d4-74313ecd64d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 06:10:26 WARN HiveExternalCatalog: The table schema given by Hive metastore(struct<transactionid:string,accountid:string,transactionamount:string,transactiondate:string,transactiontype:string,location:string,deviceid:string,ip_address:string,merchantid:string,channel:string,customerage:string,customeroccupation:string,transactionduration:string,loginattempts:string,accountbalance:string,previoustransactiondate:string>) is different from the schema when this table was created by Spark SQL(struct<TransactionID:string,AccountID:string,TransactionAmount:decimal(10,2),TransactionDate:timestamp,TransactionType:string,Location:string,DeviceID:string,IP_Address:string,MerchantID:string,Channel:string,CustomerAge:int,CustomerOccupation:string,TransactionDuration:int,LoginAttempts:int,AccountBalance:decimal(10,2),PreviousTransactionDate:timestamp>). We have to fall back to the table schema from Hive metastore which is not case preserving.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "INSERT INTO TABLE transactions_partitioned PARTITION (year = 2023)\n",
    "SELECT \n",
    "    TransactionID,\n",
    "    AccountID,\n",
    "    CAST(TransactionAmount AS DECIMAL(10,2)) AS TransactionAmount,\n",
    "    CAST(TransactionDate AS TIMESTAMP) AS TransactionDate,\n",
    "    TransactionType,\n",
    "    Location,\n",
    "    DeviceID,\n",
    "    IP_Address,\n",
    "    MerchantID,\n",
    "    Channel,\n",
    "    CAST(CustomerAge AS INT) AS CustomerAge,\n",
    "    CustomerOccupation,\n",
    "    CAST(TransactionDuration AS INT) AS TransactionDuration,\n",
    "    CAST(LoginAttempts AS INT) AS LoginAttempts,\n",
    "    CAST(AccountBalance AS DECIMAL(10,2)) AS AccountBalance,\n",
    "    CAST(PreviousTransactionDate AS TIMESTAMP) AS PreviousTransactionDate\n",
    "FROM transactions\n",
    "WHERE YEAR(CAST(TransactionDate AS TIMESTAMP)) = 2023\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa08eace-6acb-41fa-b3a0-521af22075e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 06:10:42 WARN HiveExternalCatalog: The table schema given by Hive metastore(struct<transactionid:string,accountid:string,transactionamount:string,transactiondate:string,transactiontype:string,location:string,deviceid:string,ip_address:string,merchantid:string,channel:string,customerage:string,customeroccupation:string,transactionduration:string,loginattempts:string,accountbalance:string,previoustransactiondate:string>) is different from the schema when this table was created by Spark SQL(struct<TransactionID:string,AccountID:string,TransactionAmount:decimal(10,2),TransactionDate:timestamp,TransactionType:string,Location:string,DeviceID:string,IP_Address:string,MerchantID:string,Channel:string,CustomerAge:int,CustomerOccupation:string,TransactionDuration:int,LoginAttempts:int,AccountBalance:decimal(10,2),PreviousTransactionDate:timestamp>). We have to fall back to the table schema from Hive metastore which is not case preserving.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "INSERT INTO TABLE transactions_partitioned PARTITION (year = 2024)\n",
    "SELECT \n",
    "    TransactionID,\n",
    "    AccountID,\n",
    "    CAST(TransactionAmount AS DECIMAL(10,2)) AS TransactionAmount,\n",
    "    CAST(TransactionDate AS TIMESTAMP) AS TransactionDate,\n",
    "    TransactionType,\n",
    "    Location,\n",
    "    DeviceID,\n",
    "    IP_Address,\n",
    "    MerchantID,\n",
    "    Channel,\n",
    "    CAST(CustomerAge AS INT) AS CustomerAge,\n",
    "    CustomerOccupation,\n",
    "    CAST(TransactionDuration AS INT) AS TransactionDuration,\n",
    "    CAST(LoginAttempts AS INT) AS LoginAttempts,\n",
    "    CAST(AccountBalance AS DECIMAL(10,2)) AS AccountBalance,\n",
    "    CAST(PreviousTransactionDate AS TIMESTAMP) AS PreviousTransactionDate\n",
    "FROM transactions\n",
    "WHERE YEAR(CAST(TransactionDate AS TIMESTAMP)) = 2024\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d7ae875-4653-41fe-b717-e3a9c0da5b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----------------+-------------------+---------------+-------------+--------+---------------+----------+-------+-----------+------------------+-------------------+-------------+--------------+-----------------------+\n",
      "|TransactionID|AccountID|TransactionAmount|    TransactionDate|TransactionType|     Location|DeviceID|     IP_Address|MerchantID|Channel|CustomerAge|CustomerOccupation|TransactionDuration|LoginAttempts|AccountBalance|PreviousTransactionDate|\n",
      "+-------------+---------+-----------------+-------------------+---------------+-------------+--------+---------------+----------+-------+-----------+------------------+-------------------+-------------+--------------+-----------------------+\n",
      "|     TX000001|  AC00128|            14.09|2023-04-11 16:29:14|          Debit|    San Diego| D000380| 162.198.218.92|      M015|    ATM|         70|            Doctor|                 81|            1|       5112.21|    2024-11-04 08:08:08|\n",
      "|     TX000002|  AC00455|           376.24|2023-06-27 16:44:19|          Debit|      Houston| D000051|    13.149.61.4|      M052|    ATM|         68|            Doctor|                141|            1|      13758.91|    2024-11-04 08:09:35|\n",
      "|     TX000003|  AC00019|           126.29|2023-07-10 18:16:08|          Debit|         Mesa| D000235| 215.97.143.157|      M009| Online|         19|           Student|                 56|            1|       1122.35|    2024-11-04 08:07:04|\n",
      "|     TX000004|  AC00070|           184.50|2023-05-05 16:32:11|          Debit|      Raleigh| D000187| 200.13.225.150|      M002| Online|         26|           Student|                 25|            1|       8569.06|    2024-11-04 08:09:06|\n",
      "|     TX000005|  AC00411|            13.45|2023-10-16 17:51:24|         Credit|      Atlanta| D000308|   65.164.3.100|      M091| Online|         26|           Student|                198|            1|       7429.40|    2024-11-04 08:06:39|\n",
      "|     TX000006|  AC00393|            92.15|2023-04-03 17:15:01|          Debit|Oklahoma City| D000579| 117.67.192.211|      M054|    ATM|         18|           Student|                172|            1|        781.68|    2024-11-04 08:06:36|\n",
      "|     TX000007|  AC00199|             7.08|2023-02-15 16:36:48|         Credit|      Seattle| D000241|140.212.253.222|      M019|    ATM|         37|            Doctor|                139|            1|      13316.71|    2024-11-04 08:10:09|\n",
      "|     TX000008|  AC00069|           171.42|2023-05-08 17:47:59|         Credit| Indianapolis| D000500|  92.214.76.157|      M020| Branch|         67|           Retired|                291|            1|       2796.24|    2024-11-04 08:10:55|\n",
      "|     TX000009|  AC00135|           106.23|2023-03-21 16:59:46|         Credit|      Detroit| D000690|  24.148.92.177|      M035| Branch|         51|          Engineer|                 86|            1|       9095.14|    2024-11-04 08:11:14|\n",
      "|     TX000010|  AC00385|           815.96|2023-03-31 16:06:57|          Debit|    Nashville| D000199|   32.169.88.41|      M007|    ATM|         55|            Doctor|                120|            1|       1021.88|    2024-11-04 08:06:32|\n",
      "|     TX000011|  AC00150|            17.78|2023-03-14 16:46:10|         Credit|  Albuquerque| D000205|   213.15.9.253|      M073| Online|         52|          Engineer|                 59|            1|       7599.52|    2024-11-04 08:06:45|\n",
      "|     TX000012|  AC00459|           190.02|2023-02-06 17:30:00|          Debit|      Memphis| D000589| 116.175.11.222|      M030| Online|         21|           Student|                173|            1|       1528.81|    2024-11-04 08:07:12|\n",
      "|     TX000013|  AC00392|           494.52|2023-06-07 17:21:28|         Credit|         Mesa| D000032| 210.98.198.143|      M057| Branch|         24|           Student|                111|            1|       1620.02|    2024-11-04 08:08:38|\n",
      "|     TX000014|  AC00264|           781.76|2023-11-20 16:39:15|          Debit|      Memphis| D000054|   193.83.0.183|      M025|    ATM|         26|           Student|                123|            1|        189.69|    2024-11-04 08:07:06|\n",
      "|     TX000015|  AC00085|           166.99|2023-02-13 16:53:57|          Debit|   Louisville| D000309| 188.124.181.12|      M017| Online|         18|           Student|                134|            1|        299.93|    2024-11-04 08:10:09|\n",
      "|     TX000016|  AC00270|           465.45|2023-12-12 16:23:31|          Debit|       Denver| D000466| 221.169.49.152|      M025|    ATM|         36|          Engineer|                129|            1|       3465.54|    2024-11-04 08:12:19|\n",
      "|     TX000017|  AC00317|           555.80|2023-10-30 16:52:49|         Credit|       Austin| D000671| 53.218.177.171|      M048| Branch|         19|           Student|                158|            1|       1131.26|    2024-11-04 08:07:35|\n",
      "|     TX000018|  AC00359|           492.93|2023-12-14 18:32:14|          Debit|     Columbus| D000432| 114.223.129.47|      M059| Online|         39|          Engineer|                234|            1|       5109.97|    2024-11-04 08:11:51|\n",
      "|     TX000019|  AC00242|            18.68|2023-02-21 16:56:50|         Credit|  Albuquerque| D000530| 93.218.115.132|      M054| Branch|         59|           Retired|                106|            1|       7948.37|    2024-11-04 08:12:01|\n",
      "|     TX000020|  AC00285|            71.48|2023-06-08 18:18:50|         Credit|     Columbus| D000039|  120.170.93.69|      M039| Online|         18|           Student|                169|            1|       6568.59|    2024-11-04 08:09:20|\n",
      "+-------------+---------+-----------------+-------------------+---------------+-------------+--------+---------------+----------+-------+-----------+------------------+-------------------+-------------+--------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "SELECT \n",
    "    TransactionID ,\n",
    "    AccountID ,\n",
    "    TransactionAmount ,\n",
    "    TransactionDate ,\n",
    "    TransactionType ,\n",
    "    Location ,\n",
    "    DeviceID ,\n",
    "    IP_Address ,\n",
    "    MerchantID ,\n",
    "    Channel ,\n",
    "    CustomerAge ,\n",
    "    CustomerOccupation ,\n",
    "    TransactionDuration ,\n",
    "    LoginAttempts ,\n",
    "    AccountBalance ,\n",
    "    PreviousTransactionDate\n",
    "FROM transactions_partitioned\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
